\documentclass[12pt,oneside]{article}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{color}
\begin{document}
\title{Assignment 2: CS 7641}
\author{Matthew Kirk}

\maketitle
\graphicspath{ { assets/ } }

\part{''Randomized Optimization of Neural Network Weights''}


\begin{enumerate}
\item Neural Network weights for Wine Data and Mushroom Classification
\item 4 Peaks
\item K Color
\item Another optimization problem
\end{enumerate}

the results you obtained running the algorithms on the networks: why did you get the results you did? what sort of changes might you make to each of those algorithms to improve performance? Feel free to include any supporting graphs or tables. And by "feel free to", of course, I mean "do".
a

\section*{''Randomized Hill Climbing''}

\section*{''Simulated Annealing''}

\section*{'Genetic Algorithm''}

\section*{''MIMIC''}

\section*{''Comparison of algorithms''}
a description of your optimization problems, and why you feel that they are interesting and exercise the strengths and weaknesses of each approach. Think hard about this.

\section*{''Changes to improve performance''}

analyses of your results. Why did you get the results you did? Compare and contrast the different algorithms. What sort of changes might you make to each of those algorithms to improve performance? How fast were they in terms of wall clock time? Iterations? Which algorithm performed best? How do you define best? Be creative and think of as many questions you can, and as many answers as you can. You know the drill.
 
Please note that the problems you create should be over discrete-valued parameter spaces. Bit strings are preferable.

The first problem should highlight advantages of your genetic algorithm, the second of simulated annealing, and the third of MIMIC. Be creative and thoughtful. It is not required that the problems be complicated or painful. They can be simple. For example, the 4-peaks and k-color problems are rather straightforward, but illustrate relative strengths rather neatly.

\part{''Unsupervised Learning''}
\section*{''The data Sets''}

Wine data and Mushroom data.

a discussion of your datasets, and why they're interesting: If you're using the same datasets as before at least briefly remind us of what they are so we don't have to revisit your old assignment write-up.
e

Now its time to explore unsupervised learning algorithms. This part of the assignment asks you to use some of the clustering and dimensionality reduction algorithms weve looked at in class and to revisit earlier assignments. The goal is for you to think about how these algorithms are the same as, different from, and interact with your earlier work.

The same ground rules apply for programming languages.

The Problems Given to You
\section*{''K-Means Clustering''}
\subsection*{''Picking K''}
\subsection*{''description of the kind of clusters that you got.''}
\subsection*{''Why did you get the clusters you did?''} 
\subsection*{''Do they make "sense"?''}
\subsection*{''If you used data that already had labels (for example data from a classification problem from assignment \#1) did the clusters line up with the labels? Do they otherwise line up naturally? Why or why not?''} 
\subsection*{''Can you describe how the data look in the new spaces you created with the various aglorithms?''}


\section*{''Expectation Maximization''}
\subsection*{''description of the kind of clusters that you got.''}
\subsection*{''Why did you get the clusters you did?''} 
\subsection*{''Do they make "sense"?''}
\subsection*{''If you used data that already had labels (for example data from a classification problem from assignment \#1) did the clusters line up with the labels? Do they otherwise line up naturally? Why or why not?''} 
\subsection*{''Can you describe how the data look in the new spaces you created with the various aglorithms?''}

\section*{''PCA''}
\subsection*{''Can you describe how the data look in the new spaces you created with the various aglorithms?''}
\subsection*{''For PCA, what is the distribution of eigenvalues?''}
\subsection*{'' Assuming you only generate k projections (i.e., you do dimensionality reduction), how well is the data reconstructed by the randomized projections?''}


\section*{''ICA''}
\subsection*{''Can you describe how the data look in the new spaces you created with the various aglorithms?''}
\subsection*{''For ICA, how kurtotic are the distributions?''}
\subsection*{'' Do the projection axes for ICA seem to capture anything "meaningful"?''}

\section*{''Randomized Projections''}
\subsection*{''Can you describe how the data look in the new spaces you created with the various aglorithms?''}
\subsection*{'' Assuming you only generate k projections (i.e., you do dimensionality reduction), how well is the data reconstructed by the randomized projections?''}
\subsection*{''How much variation did you get when you re-ran your RP several times (I know I don't have to mention that you might want to run RP many times to see what happens, but I hope you forgive me)?''}


\section*{''Another Feature Selection Algorithm''}
\subsection*{''Can you describe how the data look in the new spaces you created with the various aglorithms?''}


\section*{''Reproduce clustering experiments on dimension reduced data''}

Reproduce your clustering experiments, but on the data after you've run dimensionality reduction on it.
When you reproduced your clustering experiments on the datasets projected onto the new spaces created by ICA, PCA and RP, did you get the same clusters as before? Different clusters? Why? Why not?


\section*{''Rerun neural network on newly projected data''}
Apply the dimensionality reduction algorithms to one of your datasets from assignment \#1 (if you've reused the datasets from assignment \#1 to do experiments 1-3 above then you've already done this) and rerun your neural network learner on the newly projected data.


When you re-ran your neural network algorithms were there any differences in performance? Speed? Anything at all?


\section*{''Rerun the neural network on clusters as features''}
Apply the clustering algorithms to the same dataset to which you just applied the dimensionality reduction algorithms (you've probably already done this), treating the clusters as if they were new features. In other words, treat the clustering algorithms as if they were dimensionality reduction algorithms. Again, rerun your neural network learner on the newly projected data.

\section*{''Compare and contrast the different algorithms.''}

What sort of changes might you make to each of those algorithms to improve performance? How much performance was due to the problems you chose? Be creative and think of as many questions you can, and as many answers as you can. Take care to justify your analysis with data explictly.

\end{document}
